<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CPP IO library]]></title>
    <url>%2FCPP-io-library%2F</url>
    <content type="text"><![CDATA[C++ IO类 表1 IO库类型和文件 头文件 类型 iostream istream, wistream 从流中读取数据 ostream, wostream 向流中写入数据 iostream, wiostream 读写流 fstream ifstream, wifstream 从文件读取数据 ofstream, wofstrkeam 向文件写数据 fstream, wfstream 读写文件 sstream istringstream, wistringstream 从string中读取数据 ostringstream, wostringstream 向string中写入数据 tringstream, wstringstream 读写string &emsp;&emsp;iostream定义了用于读写流的基本类型，fstream定义了读写命名文件的类型， sstream定义了读写内存的类型宽字符版本的类型和函数都以一个w开始。宽字符版本的类型和对象与其对应的普通char版本的类型定义在同一个头文件中。IO类型间的关系&emsp;&emsp;类型ifstream和istringstream都继承自istream。因此，可以像使用istream一样使用ifstream和istringstream。同样，类型ofstream和ostringstream都继承自istream。 IO对象无拷贝或者赋值1234ofstream out1, out2;out1 = out2; //错误，不能对IO对象赋值ofstream print(ofstream); //错误，不能初始化ofstream对象out2 = print(out1); //错误，不能拷贝流对象 &emsp;&emsp;由于不能拷贝IO对象，因此我们也不能将形参或者返回类型设置为流类型。进行IO操作的函数通常以引用方式传递和返回流。读写一个IO对象会改变其状态，因此传递和返回的引用不能时const的。 条件状态表2 IO库条件状态|————————————————||strm::iostate | strm是一种IO类型，如表1列出。iostate是一种机器相关类型，提供了表达状态条件的完整功能。||strm::badbit | 指出流已经崩溃||strm::failbit | 指出一个IO操作失败||strm::eofbit | 指出流已经到达文件结束||strm::goodbit | 指出流未处于错误状态。此值保证为0||————————————————-|12int ival;cin &gt;&gt; ival; 如果键入Boo，则会报错，cin进入错误状态，输入一个文件结束符号标识，同样会报错；由于流可能出现错误状态，因此在使用一个流之前要注意检查流的状态，例如直接使用一个条件：12while(cin &gt;&gt; word) //...... 检查流的状态badbit:表示系统级错误，如不可恢复的错误，一旦badbit被置位，流就无法使用;failbit:当发生可恢复错误时被置位，例如期望读取一个数值却读取到一个字符，这种情况流还可以被修正后继续使用；当流到达文件结束的位置，eofbit和failbit都被置位。good位为0，表示未发生错误。管理流对象1234auto old_state = cin.rdstate(); //记住cin的状态cin.clear(); //clear不接收任何参数时，清楚所有错误标志位，使cin有效process_input(cin); //使用cincin.setstate(old_state); //将cin设置为原有状态 带参数的clear()可以接受一个iostate值，表示流的新状态 管理输出缓冲每个输出流都管理一个缓冲区，用来保存程序读写的数据。例如：1os &lt;&lt; &quot;hello world&quot;; 文本串可能被立即打印出来，也可能被操作系统保存在缓冲区，稍后打印。刷新输出缓冲区IO库中刷新缓冲区的操作主要有三个：endl:完成换行并刷新缓冲区；flush:刷新缓冲区，但是不输出任何额外的字符；ends:向缓冲区插入一个额外的空字符，然后刷新缓冲区。unitbufunitbuf使流在接下来每一次写操作之后都进行一次flush操作;而nounitbuf使流恢复到正常的系统管理的缓冲区刷新机制。12cout &lt;&lt; unitbuf; //所有输出操作之后都会立即刷新缓冲区cout &lt;&lt; nounitbufl //回到正常的刷新机制 注意：如果程序崩溃，输出缓冲区不会被刷新，它所输出的内容很可能驻留在缓冲区中等待打印。因此，在调试程序时，一定需要确定那些你认为已经输出的数据确实已经刷新了。否则，可能花大量的时间去最终程序为什么没有执行，但是实际上已经输出了，只是没有打印出来。关联输入和输出流当一个输入流关联到一个输出流的时候，任何试图从输入流读取数据的操作都会先刷新关联的输出流。标准库已经将cin和cout关联在一起，1cin &gt;&gt; ival; 导致输出流cout被刷新。]]></content>
      <tags>
        <tag>C++</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[STL allcator]]></title>
    <url>%2FSTL-allcator%2F</url>
    <content type="text"><![CDATA[空间配置器标准接口12345678allocator::value_typeallocator::pointerallocator::const_pointerallocator::referenceallocator::const_referenceallocator::size_typeallocator::difference_typeallocator::rebind // class rebind&lt;U&gt;拥有唯一成员other；是一个typedef，代表allocator&lt;U&gt; 构造函数和析构函数，由于没有数据成员，不需要被初始化，但是必须被定义。1234allocator::allocator()allocator::allocator(const allocator&amp;)template &lt;class U&gt; allocator::allocator(const allocator&lt;U&gt;&amp;)//泛化的copy constructorallocator::~allocator() 1pointer allocator::address(reference x) const 返回某个对象的地址，a.address(x)相当于&amp;x1const_pointer allocator::address(const_reference x) const 返回某的const对象的地址，a.address(x)相当于&amp;x1pointer allocator::allocate(size_type n, const void*=0) 配置空间，足以存储n个T对象。第一个参数是个提示，实现上可能利用它来增进区域性（locality），或者完全忽略之。1size_type allocator::max_size() const 返回可成功配置的最大量1void allocator::deallocate(pointer p, size_type n) 归还之前配置的空间1void allocator::construct(pointer p, const T&amp; x) 等同于new((void*) p) T(x)1void allocator::destory(pointer p) 等同于p-&gt;~T(x)]]></content>
      <categories>
        <category>STL源码剖析</category>
      </categories>
      <tags>
        <tag>STL</tag>
        <tag>空间配置器</tag>
        <tag>allocator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode 242.Valid Anagram]]></title>
    <url>%2Fleetcode-242-Valid-Anagram%2F</url>
    <content type="text"><![CDATA[242. Valid AnagramGiven two strings s and t , write a function to determine if t is an anagram of s. Example 1: Input: s = “anagram”, t = “nagaram”Output: true Example 2: Input: s = “rat”, t = “car”Output: false Note:You may assume the string contains only lowercase alphabets. Follow up:What if the inputs contain unicode characters? How would you adapt your solution to such case? 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/valid-anagram著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。&emsp;&emsp;分析：&emsp;&emsp;这道题考的是数组的问题，首先解释以下什么是字母异位词（Anagram）。如题目所述，字符串s里面含有3个a，1个n，1个g，一个r和一个m，在字符串t中含有相同的字母，并且对应字符的数目相同，因此两者互为异位词。最容易想到的一种解法就是初始化一个长度为128的辅助数组全0，用字符作为下标，首先遍历字符串s，并对s中出现的字符进行计数，之后遍历t再因此减去计数，当s和t互为异同词的时候，辅助数组值依然全0，否则不互为异位词。&emsp;&emsp;C++:1234567891011121314151617class Solution &#123;public: bool isAnagram(string s, string t) &#123; int arr[128] = &#123;0&#125;; int len1 = s.length(), len2 = t.length(); if(len1 != len2) return false; for(int i = 0; i &lt; len1; i++)&#123; arr[s[i]]++; arr[t[i]]--; &#125; for(char k = &apos;a&apos;; k &lt;= &apos;z&apos;; k++)&#123; if(arr[k] != 0) return false; &#125; return true; &#125;&#125;; &emsp;&emsp;使用C++中STL的hashmap:12345678910111213141516class Solution &#123;public: bool isAnagram(string s, string t) &#123; if(s.length() != t.length()) return false; unordered_map &lt;char, int&gt; mymap; for(char ch : s) mymap[ch]++; for(char ch : t)&#123; if(mymap[ch] &gt; 0) mymap[ch]--; else return false; &#125; return true; &#125;&#125;; &emsp;&emsp;发现使用hashmap实现更慢，这是为什么呢？]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>算法</tag>
        <tag>Valid Anagram</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode Median of Two Sorted Arrays]]></title>
    <url>%2Fleetcode-Median-of-Two-Sorted-Arrays%2F</url>
    <content type="text"><![CDATA[Median of Two Sorted Arrays&emsp;&emsp;There are two sorted arrays nums1 and nums2 of size m and n respectively.Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)).You may assume nums1 and nums2 cannot be both empty.Example 1:nums1 = [1, 3] nums2 = [2]The median is 2.0 Example 2:nums1 = [1, 2] nums2 = [3, 4]The median is (2 + 3)/2 = 2.5&emsp;&emsp;分析：&emsp;&emsp;这个题本质是一个查找问题，并且两个数组已经有顺序，可以采用折半查找。但是由于是两个数组，因此需要在两个数组之间进行混合折半查找。如果m+n为奇数，中位数为最中间的那一个数；如果m+n为偶数，中位数为中间两个数的平均值。那么，如果在两个数组中进行如上的查找呢？可以发现，不论m+n是偶数还是奇数，取第k=（m+n+1）/2个与第k=（m+n+2）/2个的平均值总能找到中位数。 1、在保证时间复杂度的同时，尽量节约空间复杂度，因此，这里不将代码进行拷贝。 2、利用两个指针i和j来指示两个数组的起始位置，如果两个i或者j已经大于了对应数组的长度，则寻求的中位数一定在另一个数组。 3、查找k时，判断每个数组里面是否有k/2个元素，如果有就将其取出；如果没有，就将median设置为最大整数（因为每次迭代需要淘汰k/2个元素，如果某个数组没有这么多元素，自然无法从该数组中淘汰元素，只能从另一个数组中淘汰元素，因此设置最大整数，保证其不会被淘汰）。&emsp;&emsp;C++：123456789101112131415161718192021222324class Solution &#123;public: double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; int len1 = nums1.size(), len2 = nums2.size(); int leftk = (len1 + len2 + 1)/2; int rightk = (len1 + len2 + 2)/2; return (findK(nums1, 0, len1, nums2, 0, len2, leftk) + findK(nums1, 0, len1, nums2, 0, len2, rightk))/2.0; &#125; int findK(vector&lt;int&gt;&amp; nums1, int i, int len1, vector&lt;int&gt;&amp; nums2, int j, int len2, int k)&#123; if(i &gt;= len1) return nums2[j + k - 1]; if(j &gt;= len2) return nums1[i + k - 1]; if(k == 1) return min(nums1[i], nums2[j]); int median1 = (i + k/2 - 1) &lt; len1 ? nums1[i + k/2 -1] : INT_MAX; int median2 = (j + k/2 - 1) &lt; len2 ? nums2[j + k/2 -1] : INT_MAX; if(median1 &lt; median2) return findK(nums1, i + k/2, len1, nums2, j, len2, k - k/2); else return findK(nums1, i, len1, nums2, j + k/2, len2, k - k/2); &#125;&#125;; &emsp;&emsp;python：123456789101112131415161718192021222324class Solution: def findK(self, nums1: List[int], i, len1, nums2: List[int], j, len2, k): if i &gt;= len1 : return nums2[j + k -1] if j &gt;= len2 : return nums1[i + k -1] if k == 1 : return min(nums1[i], nums2[j]) median1 = nums1[i + int(k/2) - 1] if (i + int(k/2) - 1) &lt; len1 else sys.maxsize median2 = nums2[j + int(k/2) - 1] if (j + int(k/2) - 1) &lt; len2 else sys.maxsize if median1 &lt; median2: return self.findK(nums1, i + int(k/2), len1, nums2, j, len2, k - int(k/2)) else: return self.findK(nums1, i, len1, nums2, j + int(k/2), len2, k - int(k/2)) def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: len1 = len(nums1) len2 = len(nums2) leftk = int((len1 + len2 + 1)/2) rightk = int((len1 + len2 + 2)/2) return (self.findK(nums1, 0, len1, nums2, 0, len2, leftk) + self.findK(nums1, 0, len1, nums2, 0, len2, rightk))/2.0 &emsp;&emsp;java：12345678910111213141516171819class Solution &#123; public double findMedianSortedArrays(int[] nums1, int[] nums2) &#123; int len1 = nums1.length, len2 = nums2.length; int leftk = (len1 + len2 + 1)/2; int rightk = (len1 + len2 + 2)/2; return (findK(nums1, 0, len1, nums2, 0, len2, leftk) + findK(nums1, 0, len1, nums2, 0, len2, rightk))/2.0; &#125; public int findK(int[] nums1, int i, int len1, int[] nums2, int j, int len2, int k)&#123; if(i &gt;= len1) return nums2[j + k - 1]; if(j &gt;= len2) return nums1[i + k - 1]; if(k == 1) return Math.min(nums1[i], nums2[j]); int median1 = (i + k/2 - 1) &lt; len1 ? nums1[i + k/2 - 1] : Integer.MAX_VALUE; int median2 = (j + k/2 - 1) &lt; len2 ? nums2[j + k/2 - 1] : Integer.MAX_VALUE; return median1 &lt; median2 ? findK(nums1, i + k/2, len1, nums2, j, len2, k - k/2) : findK(nums1, i, len1, nums2, j + k/2, len2, k - k/2); &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>算法</tag>
        <tag>Median of Two Sorted Arrays</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非极大值抑制算法原理]]></title>
    <url>%2Fnms%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;目标检测器从端到端学习范式中获益匪浅。推荐、特征和分类器成为一个神经网络将通用的目标检测的精度提高了2倍。另外一个不可或缺的组成部分就是非极大值抑制算法（non-maximum suppression,NMS)，一种后处理算法，主要用于负责合并同一对象的所有检测结果。 目标检测步骤&emsp;&emsp;所有的目标检测器都经过以下三个步骤：1）利用滑动窗口提取一个窗口的搜索空间（由许多边界框（bounding box）组成）；2）用分类器或者回归器对窗口进行置信度（confidence score）评估；3）利用非极大值抑制算法合并每一个对象的边界框。 非极大值抑制算法&emsp;&emsp;下面主要对非极大值抑制算法做一个简单的介绍。 &emsp;&emsp;如上的第一步，目前的目标检测算法会在第一步的时候，为每一个对象确定不止一个边界框，但是算法最后进行目标种类识别的时候，仅仅需要一个边界框。这个时候，我们需要去找一个最有利于目标识别的框来进行目标识别，例如下面这个例子： &emsp;&emsp;由上图可以看到算法开始可以为每一个可能的目标确定多个边界框，每个框的参数主要表示为(x,y,h,w,s)。其中(x,y)表示框的中心坐标，（h,w)表示框的高和宽，最后一个参数s指的是每个框的置信度，由上面的第二步计算得到。非极大值抑制算法需要做的就是上图从左到右的过程。 非极大值算法伪代码&emsp;&emsp;算法的伪代码如下： &emsp;&emsp;输入的是由第一步得到的边界框的集合B以及第二步计算得到的置信度集合S，$N_t$表示NMS的阈值。首先选取集合B中置信度最大的边界框，把此边界框从集合B做去除，加入到一个新集合D中，这个集合存放左右的保留框。然后将B集合中的每一个边界框与保留框做交并比（IOU），并将结果大于阈值$N_t$的边界框从B集合中剔除，直到B集合为空，此时所有的保留框都已经存放在集合D中了。真正进行目标检测的时候，使用的就是集合D中的保留框。 &emsp;&emsp;可以利用下面这个例子来体会算法的具体流程： &emsp;&emsp;我们可以看到图中每一只狗都有各自的边界框，每个框都有各自的参数(x,y,h,w,s)。首先按照置信度S对边界框进行排序，找出置信度最大的边界框，这里找到的是置信度为0.9的红色框，将红色框从原集合B中剔除，加入到集合D中。然后将置信度为0.9的红色框与剩余的边界框做IOU，得到的结果分别是0、0.6、0.05、0，并且假设阈值为0.5，可以看到此时与粉色框的IOU = 0.6，大于阈值，此时直接剔除粉色框，即将其置信度设置为0。再从剩余的框中继续选择置信度最大的边界框，这里找到是置信度为0.89的黄色框，将黄色框从集合B中剔除，加入到保留框集合D，再将其与剩余的框做IOU，结果分别为0.75和0.7，均大于阈值，直接剔除。此时集合B为空，计算结束，可以看到我们最终得到两个边界框。&emsp;&emsp;一般将阈值设置为0-0.5，阈值太大将不会有效的剔除多余的边界框，达不到过滤的效果，如下下面的这个例子： &emsp;&emsp;经过如上算法过程，可以看到保留框集合中，每一个目标都含有多个边界框，没有达到过滤得目的。因此，在使用NMS时，需要合理得设置阈值。 &emsp;&emsp;另外，如果是two stage算法，例如R-CNN系列模型，通常在选出边界框时只有（x,y,h,w,s)五个参数。这与算法得过程有关，因为算法是先选出边界框，然后将边界框与feature map做rescale,再用分类器分类。&emsp;&emsp;然而对于one stage的算法，例如YOLO，SSD。边界框除了以上(x,y,h,w,s,)这五个参数，还有对应的分类概率，因此在进行计算时不用再rescale和计算分类概率，计算量更小，速度更快。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>NMS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO实时目标检测]]></title>
    <url>%2Fyolo-real-time-object-detection%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;我们可以一眼识别出图片里面的目标和目标的位置以及目标的动作等信息。我们的视觉系统快速且准确，可以让我们几乎不用思考就可以识别复杂的目标。用于物体检测的快速，准确的算法将允许计算机在没有专用传感器的情况下驾驶汽车，使得辅助设备能够实时传送 场景信息给人类用户，并可以用于通用的响应式机器人系统。快速准确的目标检测算法的研究一直没有停下来。 &emsp;&emsp;目前比较流行的目标检测算法可以分为两类：一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN），它们是two-stage的，需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归。&emsp;&emsp;另一类是Yolo，SSD这类one-stage算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置。&emsp;&emsp;第一类方法是准确度高一些，但是速度慢，但是第二类算法是速度快，但是准确性要低一些。 &emsp;&emsp;下面将要介绍YOLO算法，You Only Look Once: Unified, Real-Time Object Detection。从算法的名字上就可以很容易知道此算法的特点，只需要进行一次CNN运算，Unified指的是这是一个统一的框架，提供end-to-end的预测，而Real-Time则时体现Yolo算法速度快。&emsp;&emsp;在介绍YOLO算法的原理之前，有必要介绍一下滑动窗口原理。 滑动窗口&emsp;&emsp;滑动窗口的思想是将目标检测问题转换为图像分类问题，即使用不同大小和比例的的窗口在整张图片上以一定的步长滑动，在每个窗口对应区域内做分类，逐步完成对整张图片的目标检测。&emsp;&emsp;很容易看出来，这个方法的问题在于无法事先知道待检测目标的的大小和位置，因此需要设置不同大小和比例的滑动窗口去进行滑动，并且需要选取合适的步长。但是此过程会产生很多的子区域，每个子区域都需要利用分类器去做预测，这就需要很大的计算量，检测速度会受到很大的影响。通常，我们又是很关注检测速度的，因此为了保证检测速度，在使用滑动窗口时，模型一般不能太复杂。&emsp;&emsp;在R-CNN中采用了selective search方法寻找最可能包含目标的子区域进行检测，即启发式过滤掉很多可能不含有目标的子区域来提高效率。在使用CNN分类器时，滑动窗口是依然是非常耗时的。为了解决这个问题，提出了全卷积的方法。 全卷积&emsp;&emsp;什么是全卷积（FCN）呢？方法很简单，结合卷积运算的特点，可以利用一种全卷积的方法实现更高效的滑动窗口。全卷积，即使用卷积层代替全连接层。例如下面这个例子，来源于吴恩达老师的网课： 假设测试图大小是16×16，并令滑动窗口大小是14×14, 滑动步长为2，所以这个测试图可以被窗口划分成4个部分，即输出的维度为[2,2,4]，可以看作4个类别的预测概率值。 可以很清楚的看出，经过这一次卷积运算就完成了滑动窗口所有子区域的分类预测，其实这就是overfeat算法的思想。这个思想来源于卷积运算的图片空间位置不变的特性，尽管卷积过程中图片大大减小了，但是特征的相对位置依然保持不变。R-CNN也借鉴了这个思想，由此产生了Fast R-CNN算法。看起来以上算法已经很好的解决了滑动窗口计算量大的问题，但是这只是对于一个固定大小，固定步长的窗口，并不能很好的在规模不同的图片在进行不同大小目标的检测。由此，产生了YOLO算法。&emsp;&emsp;[YOLO论文连接]：https://arxiv.org/abs/1506.02640 YOLO算法原理&emsp;&emsp;YOLO算法直接将原始图片分割成互不重合的小方块，然后通过卷积最后生产这样大小的特征图，基于上面的分析，可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标，这就是YOLO算法的朴素思想。 YOLO设计理念&emsp;&emsp;总的来说，YOLO算法采用一个单独的CNN模型实现end-to-end的目标检测，如下图所示： &emsp;&emsp;首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。这个过程相比R-CNN算法，其是一个统一的框架，其速度更快，而且YOLO的训练过程也是end-to-end的。 &emsp;&emsp;具体来说，YOLO的CNN网络首先将输入的图片分割成 S×S的网格，然后每个单元格负责去检测那些中心点落在该格子内的目标。如下图所示： &emsp;&emsp;如上图，可以看到狗这个目标的中心落在左下角一个单元格内，因此该单元格负责预测这个狗。每个单元格会预测B个边界框（bounding box）并计算边界框的置信度（confidence score）。这里置信度包含两个方面，一是边界框含有目标的可能性大小，二是边界框的准确度。前者记为Pr(Object)，当该边界框不包含目标时，Pr(object)=0。当该边界框包含目标时，Pr(object)=1。 &emsp;&emsp;边界框的准确度可以用预测框（predicated box）与实际框（ground truth）的交并比（IOU，intersection over union）来表示，记为$\text{IOU}^{truth}_{pred}$。因此置信度可以定义为$Pr(Object)*\text{IOU}^{truth}_{pred}$。 &emsp;&emsp;注意论文里面定义的置信度并不是边界框是否含有目标的概率，而是预测边界框的准确度与Pr(object)的乘积。 &emsp;&emsp;每一个网格单元预测C个类别的概率值，记为：$Pr(Class_i|Object)$。它表示在每个单元格里属于每一个类别的概率。不管有多少个边界框，都仅在每一个单元格上预测一组概率值（这在YOLO后面的版本做了改动）。&emsp;&emsp;现在可以利用条件类别概率与每个单元格置信度的乘积计算出每个每个边界框特定类别的置信度： Pr(Class_i|Object)*Pr(Object)*IOU^{truth}_{pred} = Pr(Class_i)*IOU^{truth}_{pred}&emsp;&emsp;边界框类别置信度表示的是该边界框中目标属于各个类别的可能性大小以及边界框匹配目标的好坏。&emsp;&emsp;原论文中利用YOLO计算PASCAL VOC数据集，利用了S=7，B=2。PASCAL VOC数据集一共有20种类别，因此YOLO最终需要预测大小为7×7×30的张量（tensor）。 网络设计&emsp;&emsp;YOLO利用卷积网络来提取特征值，再利用全连接层预测输出概率值和坐标。其网络结构受GoogleNet模型的启发，拥有24各卷积层，之后紧跟两个全连接层。 &emsp;&emsp;在YOLO模型中，对于卷积层，主要使用1x1卷积来做channle reduction，然后紧跟3x3卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数： max(x, 0.1x) 。但是最后一层却采用线性激活函数。&emsp;&emsp;在原论文中，作者还训练了加速版YOLO，用了9个卷积层代替原始的24个卷积层，同时在这些卷积层中采用了更少的卷积核。 模型的训练&emsp;&emsp;正式训练之前，利用了前20个卷积层紧跟一个平均池化层(average-pooling layer)和一个全连接层构成的分类模型在ImageNet数据集上进行了预训练。之后加上4个随机初始化的卷积层和两个随机初始化的全连接层，构成了上图的网络架构。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。模型的最后一层预测了类概率和边界框坐标。利用图片的高度和宽度对边界框的坐标进行了归一化处理，使它们的范围介于0-1之间。对边界框的坐标进行了参数化处理，其范围也是(0,1)。 &emsp;&emsp;Yolo算法将目标检测看成回归问题，所以采用的是均方差损失函数。但是对不同的部分采用了不同的权重值。首先区分定位误差和分类误差。对于定位误差，即边界框坐标预测误差，采用较大的权重$\lambda _{coord}=5$。然后其区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值 $\lambda _{noobj}=0.5$。其它权重值均设为1。然后采用均方误差，其同等对待大小不同的边界框，但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为 $(x,y,\sqrt{w}, \sqrt{h})$。 &emsp;&emsp;另外，YOLO在每个单元格预测多个边界框。但是在训练时，只需要一个边界框预测器负责每一个目标。如果该单元格内确实存在目标，那么只选择与ground truth的IOU最大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能。 但是如果一个单元格内存在多个目标怎么办呢？按照原文的设计，这时候Yolo算法就只能选择其中一个来训练，这也是Yolo算法的缺点之一。 最终YOLO的损失函数计算如下： &emsp;&emsp;其中第一项是边界框中心坐标的误差项， $1^{obj}_{ij}$ 指的是第 i 个单元格存在目标，且该单元格中的第 j 个边界框负责预测该目标。第二项是边界框的高与宽的误差项。第三项是包含目标的边界框的置信度误差项。第四项是不包含目标的边界框的置信度误差项。而最后一项是包含目标的单元格的分类误差项， $1^{obj}_{i}$ 指的是第 i 个单元格存在目标。这里特别说一下置信度的target值 C_i ，如果是不存在目标，此时由于 Pr(object)=0，那么 C_i=0 。如果存在目标， Pr(object)=1 ，此时需要确定 $\text{IOU}^{truth}_{pred}$，当然你希望最好的话，可以将IOU取1，这样 C_i=1 ，但是在YOLO实现中，使用了一个控制参数rescore（默认为1），当其为1时，IOU不是设置为1，而就是计算truth和pred之间的真实IOU。 网络的预测&emsp;&emsp;YOLO的预测用到了非极大值抑制算法(non maximum suppression, NMS)，这里对算法的原理做一个简单的介绍，链接：https://tcnull.github.io/nms/ &emsp;&emsp;了解了NMS算法再来分析YOLO的预测过程，以batch = 1为例，即只预测一张输入图片。&emsp;&emsp;由前面的分析，很容易知道网络的最终输出的张量大小是7×7×30，将其分割成三个部分：类别概率部分为 [7, 7, 20]，置信度部分为 [7,7,2] ，而边界框部分为 [7,7,2,4]。将前两部分相乘（矩阵[7, 7, 20]乘以[7,7,2] 可以各补一个维度来完成 [7,7,1,20]×[7,7,2,1]）可以得到类别置信度值为[7, 7,2,20]，即总共预测了772=98个边界框。对于这98个边界框，首先将置信度小于阈值的值置为0，然后分别对置信度值使用NMS，并且这里NMS的处理过程和传统过程不一样。这里不对NMS的结果进行剔除，而是直接将其置信度置为0。最后确定每个边界框的类别，当其置信度不为0时才输出检测结果。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>YOLO</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用MTCNN和Facenet实现实时人脸识别]]></title>
    <url>%2FuseFacenet-MTCNN-TensorFlow-Realize-Face-Recognition%2F</url>
    <content type="text"><![CDATA[MTCNN做人脸检测和对齐&emsp;&emsp;将需要识别的含有人脸的图片放在’./images/test_img’,代码首先使用detection_face_and_crop对提供的人脸图像进行预处理（人脸检测和对齐）,将预处理之后的人脸图像存放到’./images/emb_img’目录下。&emsp;&emsp;三个重要参数：&emsp;&emsp;minisize: 图片中人脸的最小尺寸，控制人脸金字塔阶数参数之一，其值越小，可计算的阶数越多，计算量越大。&emsp;&emsp;threhold: MTCNN中三个网络人脸框的阈值，三个阈值可以分别设置，这里分别设置为0.6、0.7、0.7。阈值太小将会导致人脸框太多，增加计算量；还可能导致不是人脸的图像检测为人脸。&emsp;&emsp;factor: 生成图像金字塔时候的缩放系数, 范围(0,1)，可控制图像金字塔的阶层数的参数之一，越大，阶层越多，计算量越大。&emsp;&emsp;detect_face()返回值为人脸框的坐标以及是人脸的概率。 动态分配显存123config = tf.ConfigProto()config.gpu_options.allow_growth = True # increase slowly until to max capacity when use GPU sess = tf.Session(config=config) detection_face_and_crop.py代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980from cv2 import cv2from scipy import miscimport tensorflow as tfimport numpy as npimport sysimport osimport copyimport argparseimport facenetimport align.detect_facedef main(): img_dir=&apos;./images/test_img&apos; img_path_set=[] #path of every images for file in os.listdir(img_dir): single_img=os.path.join(img_dir,file) print(&apos;loading: &#123;&#125;&apos;.format(file)) img_path_set.append(single_img) images = load_and_align_data(img_path_set, 160, 44) emb_dir=&apos;./images/emb_img&apos; if(os.path.exists(emb_dir)==False): os.mkdir(emb_dir) count=0 for file in os.listdir(img_dir): print(&quot;save &#123;&#125; &quot;.format(file)) misc.imsave(os.path.join(emb_dir,file),images[count]) count=count+1 print(&quot;get &#123;&#125; faces&quot;.format(count)) def load_and_align_data(image_paths, image_size, margin): minisize = 20 # minimum size of face threshold = [ 0.6, 0.7, 0.7 ] # threshold of bounding_boxe in three nets factor = 0.709 # scale factor of face pyramid print(&apos;Creating networks and loading parameters&apos;) with tf.Graph().as_default(): # apply video memory dynamically config = tf.ConfigProto() config.gpu_options.allow_growth = True # increase slowly until to max capacity when use GPU # config.gpu_options.per_process_gpu_memory_fraction = 0.4 # use 40% capacity of GPU sess = tf.Session(config=config) with sess.as_default(): pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None) tmp_image_paths=copy.copy(image_paths) img_list = [] for image in tmp_image_paths: print(image) img = misc.imread(os.path.expanduser(image), mode=&apos;RGB&apos;) img_size = np.asarray(img.shape)[0:2] bounding_boxes, _ = align.detect_face.detect_face(img, minisize, pnet, rnet, onet, threshold, factor) print(bounding_boxes) if len(bounding_boxes) &lt; 1: image_paths.remove(image) print(&quot;can&apos;t detect face, remove &quot;, image) continue det = np.squeeze(bounding_boxes[0,0:4]) bb = np.zeros(4, dtype=np.int32) bb[0] = np.maximum(det[0]-margin/2, 0) bb[1] = np.maximum(det[1]-margin/2, 0) bb[2] = np.minimum(det[2]+margin/2, img_size[1]) bb[3] = np.minimum(det[3]+margin/2, img_size[0]) cropped = img[bb[1]:bb[3],bb[0]:bb[2],:] # resize images deal with alignd aligned = misc.imresize(cropped, (image_size, image_size), interp=&apos;bilinear&apos;) prewhitened = facenet.prewhiten(aligned) img_list.append(prewhitened) images = np.stack(img_list) return imagesif __name__==&apos;__main__&apos;: main() 利用facenet和mtcnn做人脸识别&emsp;&emsp;利用opencv调取电脑或者网络摄像头，也可以读取视频进行识别。读取网络摄像头,当使用本地摄像头时,VideoCapture()参数设置为“0”：12video=&quot;http://admin:admin@192.168.137.33:8081/&quot;capture =cv2.VideoCapture(video) &emsp;&emsp;读取视频同时设置读入视频的宽高和帧率12345dirVideo = &quot;video1.mp4&quot;capture =cv2.VideoCapture(dirVideo)capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)capture.set(cv2.CAP_PROP_FPS, 60) &emsp;&emsp;当需要将opencv的视频输出时，使用opencv的VideoWriter()输出视频：123size = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))fourcc = cv2.VideoWriter_fourcc(*&apos;XVID&apos;)writeVideo = cv2.VideoWriter(&apos;output.avi&apos;,fourcc, 20, size, 1) &emsp;&emsp;这里以一个短视频人脸识别为例，首先将原视频输入，利用opencv获取视频帧，这里是将原视频中每三帧取一帧来做人脸距离计算。主要过程是将opencv读取的画面做人脸检测和对齐（使用MTCNN网络），再将得到的人脸图像和从emb_img中读取的包含人脸的图像做距离计算，将距离小于阈值的图像对应的标签放到列表。最后就是在视频中绘制人脸框并将对应的标签显示在框上，最终效果如下图：&emsp;&emsp;完整源代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# video=&quot;http://admin:admin@192.168.137.33:8081/&quot;# capture =cv2.VideoCapture(video)dirVideo = &quot;video1.mp4&quot;capture =cv2.VideoCapture(dirVideo)# capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)# capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)capture.set(cv2.CAP_PROP_FPS, 60)# size =(int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)))# fourcc = cv2.VideoWriter_fourcc(&apos;M&apos;,&apos;J&apos;,&apos;P&apos;,&apos;G&apos;)# writeVideo = cv2.VideoWriter(&quot;aaa.avi&quot;, fourcc, 5, size)# size = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))# fourcc = cv2.VideoWriter_fourcc(*&apos;XVID&apos;)# writeVideo = cv2.VideoWriter(&apos;output.avi&apos;,fourcc, 20, size, 1)cv2.namedWindow(&quot;camera&quot;,1)picNumber = 0count = 0frame_interval = 3while True: isSuccess, frame = capture.read() if(count % frame_interval == 0): rgb_frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) tag, bounding_box, crop_image, =load_and_align_data(rgb_frame,160,44) if(tag): feed_dict = &#123; images_placeholder: crop_image, phase_train_placeholder:False &#125; emb = sess.run(embeddings, feed_dict=feed_dict) print(emb) temp_num=len(emb) fin_obj=[] # calculate distance between camera face and in emd_img face for i in range(temp_num): dist_list=[] for j in range(compare_num): dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], compare_emb[j,:])))) dist_list.append(dist) min_value=min(dist_list) if(min_value&gt;0.65): fin_obj.append(&apos;UNKNOW&apos;) else: fin_obj.append(all_obj[dist_list.index(min_value)][0:6]) #mini distance is face which recongnition # draw rectangle for rec_position in range(temp_num): cv2.rectangle(frame, (bounding_box[rec_position,0], bounding_box[rec_position,1]), (bounding_box[rec_position,2], bounding_box[rec_position,3]), (0, 255, 0), 2, 8, 0) cv2.putText(frame, fin_obj[rec_position], (bounding_box[rec_position,0],bounding_box[rec_position,1]), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.8, (0, 0 ,255), thickness = 2, lineType = 2) # writeVideo.write(frame) cv2.imshow(&apos;camera&apos;,frame) count += 1 key = cv2.waitKey(3) if key == 27: print(&quot;ESC break&quot;) break if key == ord(&apos; &apos;): picNumber += 1 # filename = &quot;&#123;&#125;_&#123;&#125;.jpg&quot;.format(dirVideo, picNumber) filename = &quot;%s_%s.jpg&quot; % (dirVideo, picNumber) cv2.imwrite(filename,frame)capture.release()cv2.destroyWindow(&quot;camera&quot;)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>人脸检测</tag>
        <tag>MTCNN</tag>
        <tag>Facenet</tag>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度卷积神经网络的人脸检测和人脸对齐方法——MTCNN]]></title>
    <url>%2FtensorflowAndMTCNN%2F</url>
    <content type="text"><![CDATA[什么是人脸检测和人脸对齐？&emsp;&emsp;人脸检测，顾名思义，就是在检测一张图片中检测有没有人脸，找出人脸的位置。人脸检测的过程即输入一张可能含有人脸的图片，然后输出人脸的位置。但是由于人脸可能出现在原始图片中的任何地方，人脸的姿势可能都很大的区别，为了便于统一处理，需要进行人脸对齐。这个过程中需要检测人脸中的关键点，例如：眼睛、鼻子、嘴巴等等，利用这些关键点可以做仿射变换将人脸校正，消除位置、姿势带来的误差。 MTCNN（Multi-task CNN)简介&emsp;&emsp;MTCNN是一种基于深度卷积神经网络的人脸检测和人脸对齐的方法。它由三个神经网络组成，分别是P-NET、R-NET、O-NET。由于原始图片中的人脸存在不同的尺度，例如有的人脸大，有的人脸小。对应比较小的人脸，可以将其放大再进行检测；而对于比较大的人脸，可以将其缩小后再进行检测。这样就可以保证在相同的尺度下检测人脸。因此，在使用MTCNN之前需要将原始的图片缩放到不同的尺度，形成一个“图像金字塔”（如下图），紧接着将对每一个尺度的图像都利用神经网络计算一遍。 P-Net:&emsp;&emsp;P-Net网络的结构如下图：&emsp;&emsp;P-NET网络的输入是一个12×12×3的图像，P-NET的工作就是判断这个12×12的图像中是否有人脸，并且给出人脸框和关键点。网络的输出主要也三部分组成：即上图中的face classification、bounding box regression、Facial landmark localization。&emsp;&emsp;face classification是一个1×1×2的向量，即输出为两个值，分别是该图像是人脸的概率和该图像不是人脸的概率，这里利用两个值来表示，可以很方便的表示交叉熵损失。&emsp;&emsp;bounding box regression是人脸框的精确位置，即框回归。bounding box regression是一个1×1×4的向量，因为对于图像中的框的表示方法，一般是采用四个数来表示它的位置（框左上角的横坐标、框左上角的纵坐标、框的宽度、框的高度）。网络输入的图像中的人脸可能不是一个完美的矩形，可能会有左右偏移，因此需要输出当前框相对于完美人脸框的偏移，由四个变量组成。在进行框回归输出的时候，需要输出框左上角横坐标的相对偏移、框左上角纵坐标的相对偏移、框宽度的误差、框高度的误差。&emsp;&emsp;Facial landmark localization是一个1×1×10的向量，它表示的是人脸上的五个关键点：左眼、右眼、鼻子、左嘴角、右嘴角，每个关键点都有横坐标和纵坐标，因此这是一个10维向量。在实际的计算中，通过P-NET中的第一层卷积的移动，会对图像中的每一个12×12区域都做一次人脸检测。 &emsp;&emsp;下图展示了“图像金字塔”经过P-Net初步筛选后的结果，计算输出的框的大小是各不相同的，这是因为P-NET计算了图片金字塔中的每一个尺度。 &emsp;&emsp;tensorflow代码实现如下：123456789101112131415class PNet(Network): def setup(self): (self.feed(&apos;data&apos;) #pylint: disable=no-value-for-parameter, no-member .conv(3, 3, 10, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv1&apos;) .prelu(name=&apos;PReLU1&apos;) .max_pool(2, 2, 2, 2, name=&apos;pool1&apos;) .conv(3, 3, 16, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv2&apos;) .prelu(name=&apos;PReLU2&apos;) .conv(3, 3, 32, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv3&apos;) .prelu(name=&apos;PReLU3&apos;) .conv(1, 1, 2, 1, 1, relu=False, name=&apos;conv4-1&apos;) .softmax(3,name=&apos;prob1&apos;)) (self.feed(&apos;PReLU3&apos;) #pylint: disable=no-value-for-parameter .conv(1, 1, 4, 1, 1, relu=False, name=&apos;conv4-2&apos;)) R-Net:&emsp;&emsp;R-Net网络的结构如下图：&emsp;&emsp;从P-Net的结果可以看到P-NET的输出是很粗糙的，因此利用R-NET对P-NET的进行调优。R-NET的输入是24×24×3的图像，即R-NET将判断24×24×3的图像中是否含有人脸，预测关键点的位置，它的输出和P-NET一样，包括：face classification、bounding box regression、Facial landmark localization。但是这里R-NET的输入是P-NET的输出中可能为人脸的区域，将这些区域缩放成24×24×3的大小，利用R-NET作进一步判定，消除P-NET产生的一些误判，如下图： &emsp;&emsp;经过R-Net消除一些误判，人脸框将在P-Net基础上减少。&emsp;&emsp;tensorflow代码实现如下：123456789101112131415161718class RNet(Network): def setup(self): (self.feed(&apos;data&apos;) #pylint: disable=no-value-for-parameter, no-member .conv(3, 3, 28, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv1&apos;) .prelu(name=&apos;prelu1&apos;) .max_pool(3, 3, 2, 2, name=&apos;pool1&apos;) .conv(3, 3, 48, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv2&apos;) .prelu(name=&apos;prelu2&apos;) .max_pool(3, 3, 2, 2, padding=&apos;VALID&apos;, name=&apos;pool2&apos;) .conv(2, 2, 64, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv3&apos;) .prelu(name=&apos;prelu3&apos;) .fc(128, relu=False, name=&apos;conv4&apos;) .prelu(name=&apos;prelu4&apos;) .fc(2, relu=False, name=&apos;conv5-1&apos;) .softmax(1,name=&apos;prob1&apos;)) (self.feed(&apos;prelu4&apos;) #pylint: disable=no-value-for-parameter .fc(4, relu=False, name=&apos;conv5-2&apos;)) P-Net：&emsp;&emsp;O-Net网络的结构如下图：&emsp;&emsp;相对于P-NET，O-NET网络的通道数和层数更多。同之前的处理类似，进一步把得到的区域做一个缩放处理，缩放成48×48的大小，输入到最后的O-NET网络。&emsp;&emsp;经过最后的O-Net的处理，误判人脸框将进一步减少。&emsp;&emsp;tensorflow代码实现如下：123456789101112131415161718192021222324class ONet(Network): def setup(self): (self.feed(&apos;data&apos;) #pylint: disable=no-value-for-parameter, no-member .conv(3, 3, 32, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv1&apos;) .prelu(name=&apos;prelu1&apos;) .max_pool(3, 3, 2, 2, name=&apos;pool1&apos;) .conv(3, 3, 64, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv2&apos;) .prelu(name=&apos;prelu2&apos;) .max_pool(3, 3, 2, 2, padding=&apos;VALID&apos;, name=&apos;pool2&apos;) .conv(3, 3, 64, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv3&apos;) .prelu(name=&apos;prelu3&apos;) .max_pool(2, 2, 2, 2, name=&apos;pool3&apos;) .conv(2, 2, 128, 1, 1, padding=&apos;VALID&apos;, relu=False, name=&apos;conv4&apos;) .prelu(name=&apos;prelu4&apos;) .fc(256, relu=False, name=&apos;conv5&apos;) .prelu(name=&apos;prelu5&apos;) .fc(2, relu=False, name=&apos;conv6-1&apos;) .softmax(1, name=&apos;prob1&apos;)) (self.feed(&apos;prelu5&apos;) #pylint: disable=no-value-for-parameter .fc(4, relu=False, name=&apos;conv6-2&apos;)) (self.feed(&apos;prelu5&apos;) #pylint: disable=no-value-for-parameter .fc(10, relu=False, name=&apos;conv6-3&apos;)) &emsp;&emsp;MTCNN的处理过程，从P-NET到N-NET，最后再到O-NET，网络输入的图片越来越大，卷积层的通道数越来越多，内部层数也越来越多，它对人脸的识别率也越来越高。还可以看出从P-NET到O-NET的运行速度越来越慢。正是由于O-NET的速度最慢，因此不直接使用O-NET，而是先用P-NET和R-NET层层过滤，减少待判别的数量，提高网络处理的速度，降低处理时间。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>人脸检测</tag>
        <tag>人脸对齐</tag>
        <tag>MTCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式六大原则——单一职责原则]]></title>
    <url>%2FSingleResponsibilityPrinciple%2F</url>
    <content type="text"><![CDATA[单一职责原则（Single Responsibility Principle, SRP)单一职责原则定义：就一个类而言，应该仅有一个引起它变化的原因。或者说，一个类只负责一项职责。单一职责原则是实现高内聚、低耦合的指导方针，它是最简单又是最难用的原则。问题提出：假如一个类负责两项职责，R1和R2。当职责R1的需求发生变化的时候，需要对该类进行修改，这个修改可能会影响到职责R2的功能。 问题解决：这个问题的解决方案是很容易想到的，即遵循单一职责原则。分别建立两个类C1和C2，分别去实现职责R1和R2。在职责R1的需求发生变化的时候，可以直接修改类C1，而不用担心R2的功能收到影响；职责R2的需求变更亦然。 软件工程真正要做的许多内容，就是发现职责并把那些职责相互分离。 那么怎么去判断一个类拥有多个职责呢？如果你可以找到不止一个动机去修改一个类，那么说明这个类不止一个职责，这个时候就需要对这个类进行职责分离。可以利用动物觅食这个简单的例子来理解单一职责原则:12345678910111213class Animal&#123; pubilc void eat(String animal)&#123; System.out.println(animal + &quot; eat grass.&quot;); &#125;&#125;public class Client&#123; public static void main(String args[])&#123; Animal animal = new Animal(); animal.eat(&quot;Cow&quot;); animal.eat(&quot;Sheep&quot;); animal.eat(&quot;Lion&quot;); &#125;&#125; 运行结果：123Cow eat grass.Sheep eat grass.Lion eat grass. 显然，这个结果是有错误的，狮子怎么能吃草呢？因为动物除了食草动物还有食肉动物（这里举例就不考虑杂食动物了）。很容看出来，此时Animal类不仅要负责食草动物的吃，还要负责食肉动物的吃，显然违背了单一职责原则。这个时候需要对Animal类进行修改，添加食肉动物的eat方法。但是如果要遵循单一职责原则，应该将Animal类细分为Herbivore（食草动物）和Carnivore（食肉动物）。123456789101112131415161718192021class Herbivore&#123; public void eat(String herbivore)&#123; System.out.println(herbivore + &quot; eat grass.&quot;); &#125;&#125;class Carnivore&#123; public void eat(String carnivore)&#123; System.out.println(carnivore + &quot; eat meat.&quot;); &#125;&#125;public class Client&#123; public static void main(String args[])&#123; Herbivore herbivore = new Herbivore(); herbivore.eat(&quot;Cow&quot;); herbivore.eat(&quot;Sheep&quot;); Carnivore carnivore = new Carnivore(); carnivore.eat(&quot;Lion&quot;); &#125;&#125; 这个时候可能会发现这样的修改开销太大，直接在原Animal中修改eat方法，开销小得多，如：12345678910111213141516class Animal&#123; pubilc void eat(String animal)&#123; if(&quot;Lion&quot;.equals(animal)) System.out.println(animal + &quot; eat meat.&quot;); else System.out.println(animal + &quot; eat grass.&quot;); &#125;&#125;public class Client&#123; public static void main(String args[])&#123; Animal animal = new Animal(); animal.eat(&quot;Cow&quot;); animal.eat(&quot;Sheep&quot;); animal.eat(&quot;Lion&quot;); &#125;&#125; 虽然这种修改方法的开销小得多，但是隐患却很大。因为之后可能还会有老虎吃肉，鳄鱼吃肉等等，还回去修改这个类，显然这是不提倡的。当然还有另一种修改方式，可以直接在Animal类中增加食肉动物吃的方法，即：12345678910111213141516class Animal&#123; pubilc void eat(String animal)&#123; System.out.println(animal + &quot; eat grass.&quot;); &#125; public void carnivore_Eat(String animal)&#123; System.out.println(animal + &quot; eat meat.&quot;); &#125;&#125;public class Client&#123; public static void main(String args[])&#123; Animal animal = new Animal(); animal.eat(&quot;Cow&quot;); animal.eat(&quot;Sheep&quot;); animal.carnivore_Eat(&quot;Lion&quot;); &#125;&#125; 显然这样的方式也违背了原始的单一职责原则，但是它却遵循了方法上的单一职责原则，因为它不对原始的方法造成影响。在实际开发中选择原始的单一职责原则，还是方法上的单一职责原则，需要根据实际情况而定。通过上面的例子，可以看出单一职责原则的优点：1、降低类的复杂度，简化逻辑，一个类只负责一项职责；2、提高类的可读性，易于维护；3、显著降低变更风险，修改一个功能时，对其他功能的影响很小。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>软件工程</tag>
        <tag>单一职责原则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从神经元到神经网络]]></title>
    <url>%2FfromNeuronToCNNRNNAndDeepLearning%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;神经网络（neural networks)方面的研究很早就已经出现，今天“神经网络”已经是一个相当大的，多学科交叉的学科领域。神经网络已经出现了很多种类，比如CNN(Convolution Neural Network), RNN(Recurrent Neural Network)等等。下面将从神经网络的最基础的模型神经元逐步深入到CNN， RNN。 什么是神经元（neuron)?&emsp;&emsp;神经元是神经网络中最基本的成分。在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向其他神经元发送化学物质，依次改变神经元内的电位。如果神经元内的电位超过某一个阈值(threshold)，那么它就会被“激活”为“兴奋”状态，继续为其他神经元发送化学物质。&emsp;&emsp;神经网络的灵感就来源于生物神经网络，即它接受一些输入，然后给出一个输出。在机器学习中，神经网络就是一个数学占位符，它的工作就是对输入进行一个函数变换，然后给出输出。 &emsp;&emsp;在神经元模型中，该神经元收到来自其他n个神经元的通过带权连接传递的输入信号。神经元要将接受到的总输入值与神经元的阈值进行比较，然后利用激活函数(activation function)进行变换产生神经元的输出。 常见的激活函数 阶跃函数(step function)&emsp;&emsp;阶跃函数是最理想的激活函数，因为它将输入值映射为“0”或者“1”，分别对应神经元的“抑制”或者“兴奋”。 sgn(x)=\begin{cases} 1,\quad x\geq 0 \\\\ 0,\quad x]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>神经元</tag>
        <tag>激活函数</tag>
        <tag>CNN</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo配置过程中遇到的坑及解决方法]]></title>
    <url>%2FhexoErrorCollection%2F</url>
    <content type="text"><![CDATA[1、ssh -T git@github.com出现Permission denied (publickey)在创建执行ssh-keygen命令之后，会让你输入一个文件名(yourfilename)来保存key。此时的文件名需要.ssh/文件夹里文件名相同，如下面的id_rsa。123$ ssh-keygen -t rsa -C "xxxxxxx@xx.xxx"Generating public/private rsa key pair.Enter file in which to save the key (/c/Users/xxx/.ssh/id_rsa):yourfilename 方案一：直接将yourfilename设置为id_rsa使用方案一时应当cd到~/.ssh/，否则需要生成后将文件移动到~/.ssh/目录下 方案二：可以将新建的文件添加到.ssh/1ssh-add ~/.ssh/yourfilename 设置完毕后，.ssh/文件夹里面需要有三个文件：id_rsa, id_rsa, known_hosts之后执行1ssh -T git@github.com 就可以看到期望的效果了 2、hexo g后FATAL bad indentation of a mapping entry 错误1123456FATAL bad indentation of a mapping entry at line 100, column 9: repo: git@github.com:xxx/xxx.g ... ^YAMLException: bad indentation of a mapping entry at line 100, column 9: repo: git@github.com:xxxx/xxx.g ... ^ 注意”^”的位置这是因为在_config.yml文件中设置站点时的缩进问题，1234deploy: type: git repo: git@github.com:tcnull/tcnull.github.io.git branch: master 正确缩进应为：1234deploy: type: git repo: git@github.com:tcnull/tcnull.github.io.git branch: master type, repo, branch应当对齐。 3、hexo g后FATAL bad indentation of a mapping entry 错误2123456FATAL bad indentation of a mapping entry at line 100, column 9: repo: git@github.com:xxx/xxx.g ... ^YAMLException: bad indentation of a mapping entry at line 100, column 9: repo: git@github.com:xxxx/xxx.g ... ^ 注意”^”的位置出现这种情况时空格问题，冒号后面都需要一个空格，如下：1234deploy: type: git repo: git@github.com:tcnull/tcnull.github.io.git branch: master 4、关于部署之后的404问题本地测试没问题，也成功上传到github, 还是出现404首先在github上面创建仓库是需要将权限设置为public，而不是private，否则将出现404错误。 github上的repository的命名格式必须是yourname.github.io这种。其次yourname必须是你的github的用户名，否则将出现404错误。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
